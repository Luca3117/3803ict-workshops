{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sL2afF5MiigW"
      },
      "source": [
        "# Image Classification by MLP - Fashion MNIST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etAzLjZ6iiga"
      },
      "source": [
        "In this exercise, we will try to use a neural network on a simple classification task: classifying images of clothes into 10 classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqjLc_2Niigb"
      },
      "source": [
        "We will first download the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4TG1Hcpeiigb",
        "outputId": "3f7b56a8-f030-44bd-a879-687373d32981"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "\n",
        "#TODO: load dataset\n",
        "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "#TODO: Resample the dataset if needed\n",
        "# X_train = ...\n",
        "# y_train = ...\n",
        "# X_test = ...\n",
        "# y_test = ...\n",
        "\n",
        "X_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9Et9NLDiigd"
      },
      "source": [
        "This dataset contains 10 classes:\n",
        "* 0:\tT-shirt/top\n",
        "* 1:\tTrouser\n",
        "* 2:\tPullover\n",
        "* 3:\tDress\n",
        "* 4:\tCoat\n",
        "* 5:\tSandal\n",
        "* 6:\tShirt\n",
        "* 7:\tSneaker\n",
        "* 8:\tBag\n",
        "* 9:\tAnkle boot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e50TL2S-iige"
      },
      "source": [
        "Now begin by exploring the data. Try to display some images with the associated label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "6ruzCRlfiige",
        "outputId": "969f5402-aada-4d76-f6cd-5b55ac745096"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGzCAYAAABpdMNsAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAJXFJREFUeJzt3Xt0lPWdx/HPJJAhgWQAQ24SMAEEBaQVJUURoomEuEu9sKto3UXXowKhR2StHnqseGk3FbeobRFP212o3XrDw2WhLVUDCUIDXbmUetAsYFpQSBAqM0mAATK//YPDLCOJ8HtM8svl/TrnOYd55vnO883DQz48c/mOzxhjBABAG4tz3QAAoGsigAAAThBAAAAnCCAAgBMEEADACQIIAOAEAQQAcIIAAgA4QQABAJwggIB26JJLLtHf//3fu24DaFUEEADACQIIAOAEAQR0QQ0NDa5bAAggdF11dXWaPXu2LrnkEvn9fqWlpenGG2/U1q1bJUn5+fkaMWKEdu7cqeuvv15JSUm6+OKLNX/+/HMeKxwOa968eRo8eLD8fr+ys7P16KOPKhwOx2y3ePFi3XDDDUpLS5Pf79fll1+uRYsWXVC/v/zlL9WtWzd95zvfia7bvHmzJk2apEAgoKSkJE2YMEEbN26MqXvyySfl8/m0c+dO3XXXXerTp4/GjRtne7iAFtfNdQOAK9OnT9dbb72lWbNm6fLLL9fhw4e1YcMGffjhh7ryyislSZ9//rkmTZqk2267TbfffrveeustPfbYYxo5cqSKi4slSZFIRN/85je1YcMGPfDAA7rsssv05z//Wc8//7z+93//VytWrIjuc9GiRRo+fLi++c1vqlu3blq1apVmzpypSCSikpKSZnv92c9+punTp+u73/2uvv/970uS1q5dq+LiYo0ePVrz5s1TXFxcNODee+89jRkzJuYx/vEf/1FDhgzRv/3bv4lvYUG7YIAuKhAImJKSkmbvnzBhgpFkXnnllei6cDhsMjIyzJQpU6LrfvWrX5m4uDjz3nvvxdS//PLLRpLZuHFjdN3Ro0fP2U9RUZHJzc2NWTdw4EDzd3/3d8YYY1588UXj8/nMM888E70/EomYIUOGmKKiIhOJRGIePycnx9x4443RdfPmzTOSzJ133tnszwq4wFNw6LJ69+6tzZs3a//+/c1u06tXL919993R2wkJCRozZow+/vjj6LqlS5fqsssu07Bhw3To0KHocsMNN0iS1q1bF902MTEx+udgMKhDhw5pwoQJ+vjjjxUMBs/Z//z58/XQQw/p2Wef1eOPPx5dv337du3atUt33XWXDh8+HN1nQ0ODCgoKtH79ekUikZjHmj59usXRAVofT8Ghy5o/f76mTZum7OxsjR49WjfddJP++Z//Wbm5udFt+vfvL5/PF1PXp08f7dixI3p7165d+vDDD9WvX78m93Pw4MHonzdu3Kh58+apsrJSR48ejdkuGAwqEAhEb1dUVOg3v/mNHnvssZjXfc7sU5KmTZvW7M8XDAbVp0+f6O2cnJxmtwVcIIDQZd1+++267rrrtHz5cr399tt67rnn9Oyzz2rZsmXR13fi4+ObrDVnvYYSiUQ0cuRILViwoMlts7OzJUl79uxRQUGBhg0bpgULFig7O1sJCQn67W9/q+eff/6cK5bhw4fryJEj+tWvfqUHH3wwJkDObPvcc8/pa1/7WpP77dWrV8zts6++gPaAAEKXlpmZqZkzZ2rmzJk6ePCgrrzySv3gBz+IBtCFGDRokP70pz+poKDgnKuls61atUrhcFj//d//rQEDBkTXn/0U3dlSU1P11ltvady4cSooKNCGDRuUlZUV3ackpaSkqLCw8IJ7BdoTXgNCl9TY2HjOay5paWnKyso6563T53P77bfr008/1c9//vNz7jt27Fj0MzdnrqbOvnoKBoNavHhxs4/dv39/vfvuuzp27JhuvPFGHT58WJI0evRoDRo0SP/+7/+u+vr6c+o+++wzq58BcIErIHRJdXV16t+/v/7hH/5Bo0aNUq9evfTuu+/qf/7nf/SjH/3I6rH+6Z/+SW+++aamT5+udevW6dprr1VjY6M++ugjvfnmm/r973+vq666ShMnTlRCQoImT56sBx98UPX19fr5z3+utLQ0HThwoNnHHzx4sN5++23l5+erqKhIa9euVUpKin7xi1+ouLhYw4cP17333quLL75Yn376qdatW6eUlBStWrXqqx4moFURQOiSkpKSNHPmTL399ttatmyZIpGIBg8erJdeekkzZsyweqy4uDitWLFCzz//vF555RUtX75cSUlJys3N1UMPPaRLL71UkjR06FC99dZbevzxx/XII48oIyNDM2bMUL9+/fQv//IvX7qPkSNH6ne/+50KCws1efJkrVmzRvn5+aqsrNQzzzyjn/70p6qvr1dGRoby8vL04IMPej42QFvxGcMn0gAAbY/XgAAAThBAAAAnCCAAgBMEEADACQIIAOAEAQQAcKLdfQ4oEolo//79Sk5O/tKxJgCA9skYo7q6OmVlZSkurvnrnHYXQPv3748ObwQAdFz79u1T//79m72/3QVQcnKypNONp6SkOO4GAGArFAopOzs7+vu8Oa0WQAsXLtRzzz2nmpoajRo1Sj/5yU/O+Yrgppx52i0lJYUAAoAO7Hwvo7TKmxDeeOMNzZkzR/PmzdPWrVs1atQoFRUVxXwxFwCga2uVAFqwYIHuv/9+3Xvvvbr88sv18ssvKykpSf/5n//ZGrsDAHRALR5AJ06c0JYtW2K+JCsuLk6FhYWqrKw8Z/twOKxQKBSzAAA6vxYPoEOHDqmxsVHp6ekx69PT01VTU3PO9qWlpQoEAtGFd8ABQNfg/IOoc+fOVTAYjC779u1z3RIAoA20+LvgUlNTFR8fr9ra2pj1tbW1ysjIOGd7v98vv9/f0m0AANq5Fr8CSkhI0OjRo1VWVhZdF4lEVFZWprFjx7b07gAAHVSrfA5ozpw5mjZtmq666iqNGTNGL7zwghoaGnTvvfe2xu4AAB1QqwTQHXfcoc8++0xPPPGEampq9LWvfU1r1qw5540JAICuy2eMMa6bOFsoFFIgEFAwGGQSAgB0QBf6e9z5u+AAAF0TAQQAcIIAAgA4QQABAJwggAAAThBAAAAnCCAAgBMEEADACQIIAOAEAQQAcIIAAgA4QQABAJwggAAAThBAAAAnCCAAgBMEEADACQIIAOAEAQQAcIIAAgA4QQABAJwggAAAThBAAAAnCCAAgBMEEADACQIIAOAEAQQAcIIAAgA4QQABAJwggAAAThBAAAAnCCAAgBMEEADACQIIAOAEAQQAcIIAAgA4QQABAJwggAAAThBAAAAnCCAAgBMEEADACQIIAOAEAQQAcIIAAgA4QQABAJwggAAAThBAAAAnCCAAgBMEEADACQIIAOAEAQQAcIIAAgA4QQABAJzo5roB4HyMMdY1Pp+vzfblhdf+2sqbb75pXTNmzBjrmksuucS65tSpU9Y13brxq+6MSCRiXRMX1zrXKlwBAQCcIIAAAE60eAA9+eST8vl8McuwYcNaejcAgA6uVZ4YHT58uN59993/3wnPvwIAvqBVkqFbt27KyMhojYcGAHQSrfIa0K5du5SVlaXc3Fx961vf0t69e5vdNhwOKxQKxSwAgM6vxQMoLy9PS5Ys0Zo1a7Ro0SJVV1fruuuuU11dXZPbl5aWKhAIRJfs7OyWbgkA0A75TCt/8OHIkSMaOHCgFixYoPvuu++c+8PhsMLhcPR2KBRSdna2gsGgUlJSWrM1dBB8Dqjt8TmgzqstPgcUCoUUCATO+3u81f9WevfurUsvvVS7d+9u8n6/3y+/39/abQAA2plW/xxQfX299uzZo8zMzNbeFQCgA2nxAHrkkUdUUVGhv/zlL/rDH/6gW2+9VfHx8brzzjtbelcAgA6sxZ+C++STT3TnnXfq8OHD6tevn8aNG6dNmzapX79+Lb0rAEAH1uIB9Prrr7f0Q6KL8/KCvdc3E7TVmwPOfuPNhfLyWml9fb11jSTt27fPuqagoMDTvmy19zdweOHlfG1sbGyzfTGMFADQqRBAAAAnCCAAgBMEEADACQIIAOAEAQQAcIIAAgA4QQABAJwggAAAThBAAAAnCCAAgBMEEADACb4mEO1eW34jalt922ZbfQnj1KlTPdWtXr26hTtpWlt8O+dXcezYMesaL+dQcnKydU1n+JZXroAAAE4QQAAAJwggAIATBBAAwAkCCADgBAEEAHCCAAIAOEEAAQCcIIAAAE4QQAAAJwggAIATBBAAwAkCCADgRMcfp4pOz8tkay8TiaW2mzC8ePFi65ru3btb18THx1vXSNKLL75oXfPQQw9Z13iZdO6lxutxSExM9FRnKxgMWteUlZV52tfRo0eta+6++25P+zofroAAAE4QQAAAJwggAIATBBAAwAkCCADgBAEEAHCCAAIAOEEAAQCcIIAAAE4QQAAAJwggAIATBBAAwAmGkaLd8zJ8sq2GikrSoEGDrGuuueYa65ra2lrrmv79+1vXSNK2bdusa8LhsHWN3++3rmlLf/3rX61revToYV3zm9/8xrrm61//unWNJB06dMhTXWvgCggA4AQBBABwggACADhBAAEAnCCAAABOEEAAACcIIACAEwQQAMAJAggA4AQBBABwggACADhBAAEAnGAYKdq9xsZG6xqvw0jfeOMNT3W2vAy59DK4MxKJWNdI0v79+61r2vNg0ePHj3uqe/jhh1u4k6bdcMMN1jXZ2dme9uV1iGlr4AoIAOAEAQQAcMI6gNavX6/JkycrKytLPp9PK1asiLnfGKMnnnhCmZmZSkxMVGFhoXbt2tVS/QIAOgnrAGpoaNCoUaO0cOHCJu+fP3++fvzjH+vll1/W5s2b1bNnTxUVFXl+DhYA0DlZv1JbXFys4uLiJu8zxuiFF17Q448/rptvvlmS9Morryg9PV0rVqzQ1KlTv1q3AIBOo0VfA6qurlZNTY0KCwuj6wKBgPLy8lRZWdlkTTgcVigUilkAAJ1fiwZQTU2NJCk9PT1mfXp6evS+LyotLVUgEIguXt9aCADoWJy/C27u3LkKBoPRZd++fa5bAgC0gRYNoIyMDElSbW1tzPra2trofV/k9/uVkpISswAAOr8WDaCcnBxlZGSorKwsui4UCmnz5s0aO3ZsS+4KANDBWb8Lrr6+Xrt3747erq6u1vbt29W3b18NGDBAs2fP1ve//30NGTJEOTk5+t73vqesrCzdcsstLdk3AKCDsw6g999/X9dff3309pw5cyRJ06ZN05IlS/Too4+qoaFBDzzwgI4cOaJx48ZpzZo16tGjR8t1DQDo8KwDKD8/X8aYZu/3+Xx6+umn9fTTT3+lxtqzL/v5m+Pz+Vqhk47n1KlT1jVeB4t60adPH+ua1NRU65pPPvnEuiYrK8u6xstQUUkqKSnxVNdebd26tc32lZSUZF2zceNG65pZs2ZZ17Q3zt8FBwDomgggAIATBBAAwAkCCADgBAEEAHCCAAIAOEEAAQCcIIAAAE4QQAAAJwggAIATBBAAwAkCCADgBAEEAHCi7cYMdyJtNdnay9TtcDhsXRMX5+3/Id27d7euaavJ1o2NjZ7qpk6dal2TlpZmXTNmzBjrmqqqKuuaL3478YVasmSJdY2XydsFBQXWNdu2bbOuWblypXWNV16Ow4EDB6xrfvvb31rXSNJNN91kXWP7u+hCt+cKCADgBAEEAHCCAAIAOEEAAQCcIIAAAE4QQAAAJwggAIATBBAAwAkCCADgBAEEAHCCAAIAOEEAAQCc6DTDSCORiHWN1yGcp06dsq7xMlg0Pj7euiYhIcG6xutxaCtLly61rrn99ts97cvLkND8/Hzrmt69e1vXXHbZZdY1FRUV1jWSVFZWZl3jZQjnSy+9ZF2TmJhoXdOzZ0/rGkk6evSodc2JEyesa44fP25ds3r1ausaydswUtsBzBe6ffv+zQMA6LQIIACAEwQQAMAJAggA4AQBBABwggACADhBAAEAnCCAAABOEEAAACcIIACAEwQQAMAJAggA4ES7HUZqjPE0wNN2H15069ZuD5unoay7du3ytK/ly5db1/zwhz+0rhk6dKh1zc0332xdI0lXXXWVpzpbn3/+uXWNlyG4OTk51jWStHPnTuuaTz/91LrGy4DVv/3tb9Y1f/nLX6xrvAoEAtY1aWlp1jXvvfeedU17wxUQAMAJAggA4AQBBABwggACADhBAAEAnCCAAABOEEAAACcIIACAEwQQAMAJAggA4AQBBABwggACADjRbqdq+nw++Xw+q+3byscff2xd89lnn1nX/OEPf7Cu+a//+i/rmmuuuca6RpK6d+9uXXPfffdZ1zQ2NlrXpKamWtdIUnV1tXWNl6GQiYmJ1jU9evSwrunTp491jSQNGzbMuubYsWPWNbW1tdY1wWDQusbrAOGePXta1/j9fuuahIQE6xovA20lb/+e4uPjPe3rfLgCAgA4QQABAJywDqD169dr8uTJysrKks/n04oVK2Luv+eee6JPn51ZJk2a1FL9AgA6CesAamho0KhRo7Rw4cJmt5k0aZIOHDgQXV577bWv1CQAoPOxfmWuuLhYxcXFX7qN3+9XRkaG56YAAJ1fq7wGVF5errS0NA0dOlQzZszQ4cOHm902HA4rFArFLACAzq/FA2jSpEl65ZVXVFZWpmeffVYVFRUqLi5u9q1/paWlCgQC0SU7O7ulWwIAtEMt/jmgqVOnRv88cuRIXXHFFRo0aJDKy8tVUFBwzvZz587VnDlzordDoRAhBABdQKu/DTs3N1epqanavXt3k/f7/X6lpKTELACAzq/VA+iTTz7R4cOHlZmZ2dq7AgB0INZPwdXX18dczVRXV2v79u3q27ev+vbtq6eeekpTpkxRRkaG9uzZo0cffVSDBw9WUVFRizYOAOjYrAPo/fff1/XXXx+9feb1m2nTpmnRokXasWOHfvnLX+rIkSPKysrSxIkT9cwzz3iajwQA6LysAyg/P1/GmGbv//3vf/+VGvJq1apV1jXl5eWe9uVl6GIkErGu8fJZqgkTJljXHDlyxLpG8jZg1ctwTC+DJE+cOGFdI0mnTp2yrvmyfw/N8XLsvJx3AwcOtK6RvB2H48ePt0mNl8GYvXr1sq6RpJMnT1rXeDn3vAwjraurs66RpBdffNG65uw3irUkZsEBAJwggAAAThBAAAAnCCAAgBMEEADACQIIAOAEAQQAcIIAAgA4QQABAJwggAAAThBAAAAnCCAAgBMEEADACZ/xMsq3FYVCIQUCAS1cuFCJiYkXXLd+/XrrfY0fP966RpJ8Pp91TW1trXVNMBi0rgkEAtY1XiZ1S96maIdCIesaL8fByzRnr3V79uyxrvFyHNLS0qxrvEyblqRwOGxd4+XfhRdtNW1a8nbMk5KSPO3Lltdp2F7OvR07dljvIxAIKBgMfum3XHMFBABwggACADhBAAEAnCCAAABOEEAAACcIIACAEwQQAMAJAggA4AQBBABwggACADhBAAEAnCCAAABOdHPdQHM++ugj+f3+C97ey9DFP//5z9Y1krdhg16GIfbo0cO65uTJk9Y1Xgd3ejkOXv6ejh07Zl3z6aefWtdIUrdu9v8k+vTpY13j5dh52Y+XgbGSt4GfXoaRejnHvRy7nj17WtdI3vrzwsvAXa8DVlNTU61rKisrrbZvaGi4oO24AgIAOEEAAQCcIIAAAE4QQAAAJwggAIATBBAAwAkCCADgBAEEAHCCAAIAOEEAAQCcIIAAAE4QQAAAJ9rtMNJx48ZZDR1csGCB9T4+//xz6xpJ6t69u3WNl2GIiYmJ1jV9+/a1rvE6jLS+vt66JhQKWdd4GbCanJxsXSN5+7v1MhTSy7DPffv2Wdc0NjZa10hSIBBok31FIhHrGi8DY+vq6qxrvNbFxdn/v97Lz+TlHJK8/XsvLy+32v5Chw5zBQQAcIIAAgA4QQABAJwggAAAThBAAAAnCCAAgBMEEADACQIIAOAEAQQAcIIAAgA4QQABAJwggAAATviMMcZ1E2cLhUIKBAIKBoNKSUm54Lply5ZZ7+u1116zrpG8DYU8duyYdY2XQY0+n8+6xuY4n83LsFQvw0i98DL8VfI24LGtanr06GFd42WQq+Tt3PPyq8RLjd/vt67x8u9CksLhsHWNl5/Jy368DDCVpAEDBljXvPDCC1bb19fXa/To0ef9Pc4VEADACQIIAOCEVQCVlpbq6quvVnJystLS0nTLLbeoqqoqZpvjx4+rpKREF110kXr16qUpU6aotra2RZsGAHR8VgFUUVGhkpISbdq0Se+8845OnjypiRMnqqGhIbrNww8/rFWrVmnp0qWqqKjQ/v37ddttt7V44wCAjs3qVaw1a9bE3F6yZInS0tK0ZcsWjR8/XsFgUP/xH/+hV199VTfccIMkafHixbrsssu0adMmfeMb32i5zgEAHdpXeg0oGAxK+v+vgd6yZYtOnjypwsLC6DbDhg3TgAEDVFlZ2eRjhMNhhUKhmAUA0Pl5DqBIJKLZs2fr2muv1YgRIyRJNTU1SkhIUO/evWO2TU9PV01NTZOPU1paqkAgEF2ys7O9tgQA6EA8B1BJSYk++OADvf7661+pgblz5yoYDEYXL5+xAQB0PJ4+yTRr1iytXr1a69evV//+/aPrMzIydOLECR05ciTmKqi2tlYZGRlNPpbf7/f0wTIAQMdmdQVkjNGsWbO0fPlyrV27Vjk5OTH3jx49Wt27d1dZWVl0XVVVlfbu3auxY8e2TMcAgE7B6gqopKREr776qlauXKnk5OTo6zqBQECJiYkKBAK67777NGfOHPXt21cpKSn69re/rbFjx/IOOABADKsAWrRokSQpPz8/Zv3ixYt1zz33SJKef/55xcXFacqUKQqHwyoqKtJLL73UIs0CADqPTjOMtC2d/cHbC7V06VLrmj/+8Y/WNWc//XmhhgwZYl0jSQcPHrSu8TKE08sp6mX4q+Rt0Ow111xjXbNz507rGi/DX73yMrwzPj7euub48ePWNV4GrHod3Hnq1Cnrmi++C/hCjBw50rrmzGctbU2fPt1TnY0L/T3OLDgAgBMEEADACQIIAOAEAQQAcIIAAgA4QQABAJwggAAAThBAAAAnCCAAgBMEEADACQIIAOAEAQQAcIIAAgA40WmmYTc2Nlrvy8v0Xnw1W7duta6JRCLWNV6mGEtSbm6udU1aWpqnfcEbL+eDlwnakvi2Zo+Yhg0AaNcIIACAEwQQAMAJAggA4AQBBABwggACADhBAAEAnCCAAABOEEAAACcIIACAEwQQAMAJAggA4EQ31w20FAaLdgxXXnml6xbQwcXF2f+/maGi7RNXQAAAJwggAIATBBAAwAkCCADgBAEEAHCCAAIAOEEAAQCcIIAAAE4QQAAAJwggAIATBBAAwAkCCADgBAEEAHCCAAIAOEEAAQCcIIAAAE4QQAAAJwggAIATBBAAwAkCCADgBAEEAHCCAAIAOEEAAQCcIIAAAE4QQAAAJwggAIATBBAAwAkCCADghFUAlZaW6uqrr1ZycrLS0tJ0yy23qKqqKmab/Px8+Xy+mGX69Okt2jQAoOOzCqCKigqVlJRo06ZNeuedd3Ty5ElNnDhRDQ0NMdvdf//9OnDgQHSZP39+izYNAOj4utlsvGbNmpjbS5YsUVpamrZs2aLx48dH1yclJSkjI6NlOgQAdEpf6TWgYDAoSerbt2/M+l//+tdKTU3ViBEjNHfuXB09erTZxwiHwwqFQjELAKDzs7oCOlskEtHs2bN17bXXasSIEdH1d911lwYOHKisrCzt2LFDjz32mKqqqrRs2bImH6e0tFRPPfWU1zYAAB2UzxhjvBTOmDFDv/vd77Rhwwb179+/2e3Wrl2rgoIC7d69W4MGDTrn/nA4rHA4HL0dCoWUnZ2tYDColJQUL60BABwKhUIKBALn/T3u6Qpo1qxZWr16tdavX/+l4SNJeXl5ktRsAPn9fvn9fi9tAAA6MKsAMsbo29/+tpYvX67y8nLl5OSct2b79u2SpMzMTE8NAgA6J6sAKikp0auvvqqVK1cqOTlZNTU1kqRAIKDExETt2bNHr776qm666SZddNFF2rFjhx5++GGNHz9eV1xxRav8AACAjsnqNSCfz9fk+sWLF+uee+7Rvn37dPfdd+uDDz5QQ0ODsrOzdeutt+rxxx+/4NdzLvS5QwBA+9QqrwGdL6uys7NVUVFh85AAgC6KWXAAACcIIACAEwQQAMAJAggA4AQBBABwggACADhBAAEAnCCAAABOEEAAACcIIACAEwQQAMAJAggA4AQBBABwggACADhBAAEAnCCAAABOEEAAACcIIACAEwQQAMAJAggA4AQBBABwggACADhBAAEAnCCAAABOEEAAACe6uW7gi4wxkqRQKOS4EwCAF2d+f5/5fd6cdhdAdXV1kqTs7GzHnQAAvoq6ujoFAoFm7/eZ80VUG4tEItq/f7+Sk5Pl8/li7guFQsrOzta+ffuUkpLiqEP3OA6ncRxO4zicxnE4rT0cB2OM6urqlJWVpbi45l/paXdXQHFxcerfv/+XbpOSktKlT7AzOA6ncRxO4zicxnE4zfVx+LIrnzN4EwIAwAkCCADgRIcKIL/fr3nz5snv97tuxSmOw2kch9M4DqdxHE7rSMeh3b0JAQDQNXSoKyAAQOdBAAEAnCCAAABOEEAAACcIIACAEx0mgBYuXKhLLrlEPXr0UF5env74xz+6bqnNPfnkk/L5fDHLsGHDXLfV6tavX6/JkycrKytLPp9PK1asiLnfGKMnnnhCmZmZSkxMVGFhoXbt2uWm2VZ0vuNwzz33nHN+TJo0yU2zraS0tFRXX321kpOTlZaWpltuuUVVVVUx2xw/flwlJSW66KKL1KtXL02ZMkW1tbWOOm4dF3Ic8vPzzzkfpk+f7qjjpnWIAHrjjTc0Z84czZs3T1u3btWoUaNUVFSkgwcPum6tzQ0fPlwHDhyILhs2bHDdUqtraGjQqFGjtHDhwibvnz9/vn784x/r5Zdf1ubNm9WzZ08VFRXp+PHjbdxp6zrfcZCkSZMmxZwfr732Wht22PoqKipUUlKiTZs26Z133tHJkyc1ceJENTQ0RLd5+OGHtWrVKi1dulQVFRXav3+/brvtNoddt7wLOQ6SdP/998ecD/Pnz3fUcTNMBzBmzBhTUlISvd3Y2GiysrJMaWmpw67a3rx588yoUaNct+GUJLN8+fLo7UgkYjIyMsxzzz0XXXfkyBHj9/vNa6+95qDDtvHF42CMMdOmTTM333yzk35cOXjwoJFkKioqjDGn/+67d+9uli5dGt3mww8/NJJMZWWlqzZb3RePgzHGTJgwwTz00EPumroA7f4K6MSJE9qyZYsKCwuj6+Li4lRYWKjKykqHnbmxa9cuZWVlKTc3V9/61re0d+9e1y05VV1drZqampjzIxAIKC8vr0ueH+Xl5UpLS9PQoUM1Y8YMHT582HVLrSoYDEqS+vbtK0nasmWLTp48GXM+DBs2TAMGDOjU58MXj8MZv/71r5WamqoRI0Zo7ty5Onr0qIv2mtXupmF/0aFDh9TY2Kj09PSY9enp6froo48cdeVGXl6elixZoqFDh+rAgQN66qmndN111+mDDz5QcnKy6/acqKmpkaQmz48z93UVkyZN0m233aacnBzt2bNH3/3ud1VcXKzKykrFx8e7bq/FRSIRzZ49W9dee61GjBgh6fT5kJCQoN69e8ds25nPh6aOgyTdddddGjhwoLKysrRjxw499thjqqqq0rJlyxx2G6vdBxD+X3FxcfTPV1xxhfLy8jRw4EC9+eabuu+++xx2hvZg6tSp0T+PHDlSV1xxhQYNGqTy8nIVFBQ47Kx1lJSU6IMPPugSr4N+meaOwwMPPBD988iRI5WZmamCggLt2bNHgwYNaus2m9Tun4JLTU1VfHz8Oe9iqa2tVUZGhqOu2ofevXvr0ksv1e7du1234syZc4Dz41y5ublKTU3tlOfHrFmztHr1aq1bty7m+8MyMjJ04sQJHTlyJGb7zno+NHccmpKXlydJ7ep8aPcBlJCQoNGjR6usrCy6LhKJqKysTGPHjnXYmXv19fXas2ePMjMzXbfiTE5OjjIyMmLOj1AopM2bN3f58+OTTz7R4cOHO9X5YYzRrFmztHz5cq1du1Y5OTkx948ePVrdu3ePOR+qqqq0d+/eTnU+nO84NGX79u2S1L7OB9fvgrgQr7/+uvH7/WbJkiVm586d5oEHHjC9e/c2NTU1rltrU//6r/9qysvLTXV1tdm4caMpLCw0qamp5uDBg65ba1V1dXVm27ZtZtu2bUaSWbBggdm2bZv561//aowx5oc//KHp3bu3WblypdmxY4e5+eabTU5Ojjl27JjjzlvWlx2Huro688gjj5jKykpTXV1t3n33XXPllVeaIUOGmOPHj7tuvcXMmDHDBAIBU15ebg4cOBBdjh49Gt1m+vTpZsCAAWbt2rXm/fffN2PHjjVjx4512HXLO99x2L17t3n66afN+++/b6qrq83KlStNbm6uGT9+vOPOY3WIADLGmJ/85CdmwIABJiEhwYwZM8Zs2rTJdUtt7o477jCZmZkmISHBXHzxxeaOO+4wu3fvdt1Wq1u3bp2RdM4ybdo0Y8zpt2J/73vfM+np6cbv95uCggJTVVXltulW8GXH4ejRo2bixImmX79+pnv37mbgwIHm/vvv73T/SWvq55dkFi9eHN3m2LFjZubMmaZPnz4mKSnJ3HrrrebAgQPumm4F5zsOe/fuNePHjzd9+/Y1fr/fDB482HznO98xwWDQbeNfwPcBAQCcaPevAQEAOicCCADgBAEEAHCCAAIAOEEAAQCcIIAAAE4QQAAAJwggAIATBBAAwAkCCADgBAEEAHDi/wCTlHr3b3AudwAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# TODO: Explore the data, display some input images\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "label_class = ['top', 'trouser', 'pullover', 'dress', 'coat', 'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']\n",
        "\n",
        "# np.random.seed(0)\n",
        "idx = np.random.randint(X_train.shape[0])\n",
        "\n",
        "plt.imshow(X_train[idx], cmap=\"gray_r\")\n",
        "plt.title(label_class[y_train[idx]])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiXVu33Ziige"
      },
      "source": [
        "**Before going further**: what methods could you use to perform such a classification task?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-TWHqBGiige"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtFJb0rPiigf"
      },
      "source": [
        "The first method you will try is using neural networks. First step is the data preparation: data rescaling, label preparation.\n",
        "\n",
        "Hint: you can use the Keras function `to_categorical`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Iwc3TEKbiigf"
      },
      "outputs": [],
      "source": [
        "# TODO: Make the data preparation\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "y_train_cat = to_categorical(y_train, num_classes=10)\n",
        "y_test_cat = to_categorical(y_test, num_classes=10)\n",
        "\n",
        "X_train_norm = X_train / 255\n",
        "X_test_norm = X_test / 255\n",
        "\n",
        "# TODO: reshape the image data (2D array) into input 1D array for a neural network\n",
        "X_train_norm = X_train_norm.reshape(X_train_norm.shape[0], np.prod(X_train_norm.shape[1:]))\n",
        "X_test_norm = X_test_norm.reshape(X_test_norm.shape[0], np.prod(X_test_norm.shape[1:]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRdtxok2iigf"
      },
      "source": [
        "Next step: model building with Keras. Build your neural network architecture. At first, I would recommend a light architecture: no more than 2 hidden layers, with about 10 units per layer. Put that model into a function, so that you can reuse it later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "uOXbxF2Siigf",
        "outputId": "fa97a2ee-53bf-4274-a688-a446aa0ed5d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m7,850\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m110\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m110\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">7,850</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">110</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">110</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m8,070\u001b[0m (31.52 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,070</span> (31.52 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m8,070\u001b[0m (31.52 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,070</span> (31.52 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# TODO: Build your model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "def my_model(input_dim):\n",
        "    # Create the Sequential object\n",
        "    model = Sequential()\n",
        "\n",
        "    # Add 2 dense layers with 10 neurons each using sigmoid or relu activation\n",
        "    model.add(Dense(10, input_dim=input_dim, activation=\"sigmoid\"))\n",
        "    model.add(Dense(10, activation=\"sigmoid\"))\n",
        "\n",
        "    # Add the output layer with one unit: the predicted result\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "    return model\n",
        "\n",
        "my_model(X_train_norm.shape[1]).summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVMiDh9giigf"
      },
      "source": [
        "Now compile and fit your model on your training data. Since this is a multiclass classification, the loss is not `binary_crossentropy` anymore, but `categorical_crossentropy`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0ZCI6ATiigf",
        "outputId": "12c9ccee-93c5-48d5-9958-efd189b7bdf7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.2587 - loss: 2.1545\n",
            "Epoch 2/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6333 - loss: 1.4557\n",
            "Epoch 3/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7184 - loss: 1.0546\n",
            "Epoch 4/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7377 - loss: 0.8143\n",
            "Epoch 5/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7551 - loss: 0.6921\n",
            "Epoch 6/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7753 - loss: 0.6180\n",
            "Epoch 7/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7857 - loss: 0.5769\n",
            "Epoch 8/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7975 - loss: 0.5482\n",
            "Epoch 9/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8153 - loss: 0.5152\n",
            "Epoch 10/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8345 - loss: 0.4893\n",
            "Epoch 11/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8450 - loss: 0.4611\n",
            "Epoch 12/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8516 - loss: 0.4447\n",
            "Epoch 13/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8538 - loss: 0.4354\n",
            "Epoch 14/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8585 - loss: 0.4169\n",
            "Epoch 15/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8604 - loss: 0.4115\n",
            "Epoch 16/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8591 - loss: 0.4123\n",
            "Epoch 17/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8593 - loss: 0.4072\n",
            "Epoch 18/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8662 - loss: 0.3911\n",
            "Epoch 19/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8665 - loss: 0.3920\n",
            "Epoch 20/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8665 - loss: 0.3846\n",
            "Epoch 21/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8689 - loss: 0.3812\n",
            "Epoch 22/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8714 - loss: 0.3754\n",
            "Epoch 23/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8721 - loss: 0.3715\n",
            "Epoch 24/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8727 - loss: 0.3641\n",
            "Epoch 25/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8724 - loss: 0.3673\n",
            "Epoch 26/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8726 - loss: 0.3675\n",
            "Epoch 27/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8756 - loss: 0.3578\n",
            "Epoch 28/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8745 - loss: 0.3611\n",
            "Epoch 29/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8752 - loss: 0.3581\n",
            "Epoch 30/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8765 - loss: 0.3532\n",
            "Epoch 31/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8761 - loss: 0.3514\n",
            "Epoch 32/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8778 - loss: 0.3507\n",
            "Epoch 33/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8796 - loss: 0.3475\n",
            "Epoch 34/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8781 - loss: 0.3474\n",
            "Epoch 35/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8789 - loss: 0.3437\n",
            "Epoch 36/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8762 - loss: 0.3497\n",
            "Epoch 37/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8793 - loss: 0.3415\n",
            "Epoch 38/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8799 - loss: 0.3445\n",
            "Epoch 39/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8808 - loss: 0.3424\n",
            "Epoch 40/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8820 - loss: 0.3389\n",
            "Epoch 41/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8808 - loss: 0.3348\n",
            "Epoch 42/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8818 - loss: 0.3322\n",
            "Epoch 43/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8813 - loss: 0.3401\n",
            "Epoch 44/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8816 - loss: 0.3295\n",
            "Epoch 45/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8824 - loss: 0.3353\n",
            "Epoch 46/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8829 - loss: 0.3359\n",
            "Epoch 47/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.8858 - loss: 0.3259\n",
            "Epoch 48/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8831 - loss: 0.3312\n",
            "Epoch 49/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8863 - loss: 0.3225\n",
            "Epoch 50/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8844 - loss: 0.3295\n",
            "Epoch 51/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8854 - loss: 0.3271\n",
            "Epoch 52/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8847 - loss: 0.3276\n",
            "Epoch 53/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8877 - loss: 0.3204\n",
            "Epoch 54/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8856 - loss: 0.3272\n",
            "Epoch 55/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8843 - loss: 0.3284\n",
            "Epoch 56/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8873 - loss: 0.3193\n",
            "Epoch 57/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8858 - loss: 0.3243\n",
            "Epoch 58/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8860 - loss: 0.3227\n",
            "Epoch 59/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8884 - loss: 0.3158\n",
            "Epoch 60/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8883 - loss: 0.3153\n",
            "Epoch 61/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8882 - loss: 0.3133\n",
            "Epoch 62/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8846 - loss: 0.3230\n",
            "Epoch 63/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8885 - loss: 0.3152\n",
            "Epoch 64/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8894 - loss: 0.3161\n",
            "Epoch 65/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8906 - loss: 0.3096\n",
            "Epoch 66/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8881 - loss: 0.3141\n",
            "Epoch 67/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8905 - loss: 0.3084\n",
            "Epoch 68/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8909 - loss: 0.3089\n",
            "Epoch 69/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8887 - loss: 0.3106\n",
            "Epoch 70/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8924 - loss: 0.3029\n",
            "Epoch 71/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8907 - loss: 0.3089\n",
            "Epoch 72/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8923 - loss: 0.3059\n",
            "Epoch 73/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8921 - loss: 0.3092\n",
            "Epoch 74/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8903 - loss: 0.3113\n",
            "Epoch 75/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8924 - loss: 0.3068\n",
            "Epoch 76/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8934 - loss: 0.3050\n",
            "Epoch 77/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8916 - loss: 0.3062\n",
            "Epoch 78/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8899 - loss: 0.3071\n",
            "Epoch 79/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8917 - loss: 0.3064\n",
            "Epoch 80/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8916 - loss: 0.3090\n",
            "Epoch 81/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8931 - loss: 0.3036\n",
            "Epoch 82/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8920 - loss: 0.3072\n",
            "Epoch 83/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8928 - loss: 0.3003\n",
            "Epoch 84/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8927 - loss: 0.3069\n",
            "Epoch 85/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8958 - loss: 0.2982\n",
            "Epoch 86/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8949 - loss: 0.2996\n",
            "Epoch 87/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8902 - loss: 0.3034\n",
            "Epoch 88/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8947 - loss: 0.2976\n",
            "Epoch 89/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8965 - loss: 0.2949\n",
            "Epoch 90/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.8956 - loss: 0.2986\n",
            "Epoch 91/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.8942 - loss: 0.2971\n",
            "Epoch 92/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8925 - loss: 0.3031\n",
            "Epoch 93/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8929 - loss: 0.3017\n",
            "Epoch 94/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8909 - loss: 0.3075\n",
            "Epoch 95/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8937 - loss: 0.2954\n",
            "Epoch 96/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8939 - loss: 0.3005\n",
            "Epoch 97/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8959 - loss: 0.2930\n",
            "Epoch 98/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8947 - loss: 0.2930\n",
            "Epoch 99/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8939 - loss: 0.2962\n",
            "Epoch 100/100\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8948 - loss: 0.2961\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7d1e07bce910>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "#https://stackoverflow.com/questions/53014306/error-15-initializing-libiomp5-dylib-but-found-libiomp5-dylib-already-initial\n",
        "# os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
        "\n",
        "# TODO: Compile and fit your model\n",
        "model = my_model(X_train_norm.shape[1])\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train_norm, y_train_cat, epochs=100, batch_size=128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bO3mgNBiigg"
      },
      "source": [
        "Once your model has been trained, compute the accuracy (and other metrics if you want) on the train and test dataset.\n",
        "\n",
        "Be careful, Keras returns softmax output (so an array of 10 values between 0 and 1, for which the sum is equal to 1). To compute correctly the accuracy, you have to convert that array into a categorical array with zeros and a 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhnoNRwfiigg",
        "outputId": "641edd3d-cb4c-4bf8-8bee-7a66cde5caf1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy on train with NN: 0.8952500224113464\n",
            "accuracy on test with NN: 0.8543999791145325\n"
          ]
        }
      ],
      "source": [
        "# TODO: Compute the accuracy of your model\n",
        "print('accuracy on train with NN:', model.evaluate(X_train_norm, y_train_cat, verbose=0)[1])\n",
        "print('accuracy on test with NN:', model.evaluate(X_test_norm, y_test_cat, verbose=0)[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svNooo-Yiigg"
      },
      "source": [
        "What do you think of those results? Can you improve it by changing the number of layers? Of units per layer? The number of epochs? The activation functions?\n",
        "\n",
        "You should try!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRjcrSOZiigg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gBWlEBfiigg"
      },
      "source": [
        "In order to compare your results with more traditional machine learning methods, you will do this work with another method: a PCA followed by a classification model (of your choice). Of course, you can perform hyperparameter optimization using a gridsearch on that model!\n",
        "\n",
        "Fit your model and display the performances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "IR_Q7Hsviigh"
      },
      "outputs": [],
      "source": [
        "# TODO: Redo the classification with PCA and classification model\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=0.9)\n",
        "\n",
        "pca.fit(X_train_norm)\n",
        "X_train_pca = pca.transform(X_train_norm)\n",
        "X_test_pca = pca.transform(X_test_norm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEAjqXvriigh",
        "outputId": "b5ae5bb5-f2a3-44db-c180-a97a736eb0ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "score with RF on train 1.0\n",
            "score with RF on test 0.8598\n"
          ]
        }
      ],
      "source": [
        "# TODO: use any classifier you want\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf = RandomForestClassifier()\n",
        "\n",
        "rf.fit(X_train_pca, y_train)\n",
        "\n",
        "print('score with RF on train', rf.score(X_train_pca, y_train))\n",
        "print('score with RF on test', rf.score(X_test_pca, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHe-6oNBiigh"
      },
      "source": [
        "Are the performances different? Can you explain why?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first performance score is using the classifier on the training data and the second score is the result for the testing data. The training data scored a perfect score of 1.0 or 100% compared to the score of 0.8598 or 85.98% accuracy for the testing data. This difference in accuracy was caused by overfitting of the classifier to the training data."
      ],
      "metadata": {
        "id": "AxBed_YCGq39"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJaNZzPOiigh"
      },
      "source": [
        "If you still have time, you could try to use scikit-learn's `Pipeline` to perform the hyperparameter optimization jointly on the PCA and the classification model. This might improve your performances."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}